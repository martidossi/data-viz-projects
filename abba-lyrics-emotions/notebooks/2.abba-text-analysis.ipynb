{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook focuses on the **text-based analysis** carried out on song lyrics and consists of two main parts:\n",
    "1. Texts preprocessing with the objective of computing statistics such as the lexical diversity and lexical density\n",
    "2. Emotion-based model application with the aim of getting emotion scores for any song\n",
    "\n",
    "Some documentation:\n",
    "- https://towardsdatascience.com/what-songs-tell-us-about-text-mining-with-lyrics-ca80f98b3829\n",
    "- The model used is a deep neural network from **HuggingFace** (*bert-base-uncased-emotion*)\n",
    "    - https://huggingface.co/bhadresh-savani/bert-base-uncased-emotion\n",
    "    - https://huggingface.co/datasets/viewer/?dataset=emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:51:44.485573Z",
     "start_time": "2021-10-30T16:51:44.474439Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import re # library for regular expression operations\n",
    "import string # for string operations\n",
    "\n",
    "# Stop words:\n",
    "from nltk.corpus import stopwords  \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenization:\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Lemmatizer:\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Bigrams:\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# HuggingFace:\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:51:45.459832Z",
     "start_time": "2021-10-30T16:51:45.414913Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/df_abba_charts.csv')\n",
    "print(f'n_rows: {df.shape[0]}, n_columns: {df.shape[1]}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:25:01.045500Z",
     "start_time": "2021-10-30T16:25:01.041476Z"
    }
   },
   "source": [
    "## 1. Texts preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T15:24:00.518597Z",
     "start_time": "2021-10-30T15:24:00.513197Z"
    }
   },
   "source": [
    "### 1.1 Texts exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:51:46.086033Z",
     "start_time": "2021-10-30T16:51:46.081396Z"
    }
   },
   "outputs": [],
   "source": [
    "# All song lyrics end with a code such as '1EmbedShare URLCopyEmbedCopy': it has to be deleted\n",
    "df.lyrics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:51:46.710237Z",
     "start_time": "2021-10-30T16:51:46.704871Z"
    }
   },
   "outputs": [],
   "source": [
    "# Join all the texts to find particular patterns or words, such as s.o.s, which are then to be amended properly\n",
    "all_texts = ' '.join(df.lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:51:47.218296Z",
     "start_time": "2021-10-30T16:51:47.182624Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find patterns defined as: some letters, dot, some letters (and more)\n",
    "p = re.compile(\"[a-z]+(?:\\.[a-z]+)+\")\n",
    "set(p.findall(all_texts.lower()))\n",
    "# Will be replaced by the string without dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:51:47.711443Z",
     "start_time": "2021-10-30T16:51:47.681264Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find patterns defined as: some letters, -, some letters (and more)\n",
    "p = re.compile(\"[a-z]+(?:-[a-z]+)+\")\n",
    "set(p.findall(all_texts.lower()))\n",
    "# These cases are helpful to identify bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:51:48.189649Z",
     "start_time": "2021-10-30T16:51:48.185265Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a dict of patterns that have to be replaced and the related replacements\n",
    "dict_replace = {\n",
    "    's.o.s': 'sos',\n",
    "    '\\d?embedshare urlcopyembedcopy': '',\n",
    "}\n",
    "# Replacements function:\n",
    "def replace_words(text, dict_replacements):\n",
    "    for key, value in dict_replacements.items():\n",
    "        text = re.sub(key, value, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T15:29:06.904493Z",
     "start_time": "2021-10-30T15:29:06.899935Z"
    }
   },
   "source": [
    "### 1.2 Texts cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:51:48.681312Z",
     "start_time": "2021-10-30T16:51:48.670143Z"
    }
   },
   "outputs": [],
   "source": [
    "# See the English stop words list\n",
    "print(len(stop_words))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:51:49.213759Z",
     "start_time": "2021-10-30T16:51:49.205092Z"
    }
   },
   "outputs": [],
   "source": [
    "# Given a song lyrics, the following function performs a number of cleaning steps as for the order below:\n",
    "# 1. lowercasing\n",
    "# 2. replacements of the words previously identified\n",
    "# 3. punctuation removal\n",
    "# 4. remove multiple spaces\n",
    "# 5. remove spaces at the beginning and at the end of each song lyrics\n",
    "# 6. Tokenization\n",
    "# 7. Stop words removal\n",
    "# 8. Lemmatization\n",
    "def cleaning_lyrics(text):\n",
    "    text = text.lower()\n",
    "    text = replace_words(text, dict_replacements=dict_replace)\n",
    "    for el in string.punctuation:\n",
    "        text = re.sub(f'\\\\{el}', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    text_tokens = word_tokenize(text)\n",
    "    text_tokens_stop_words = [i for i in text_tokens if not i in stop_words]\n",
    "    text_tokens_lemm = [wordnet_lemmatizer.lemmatize(word, pos='v') for word in text_tokens_stop_words]\n",
    "    return text_tokens_lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:51:49.932193Z",
     "start_time": "2021-10-30T16:51:49.672750Z"
    }
   },
   "outputs": [],
   "source": [
    "df['cleaned_lyrics'] = df.lyrics.apply(lambda x: cleaning_lyrics(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:51:50.698429Z",
     "start_time": "2021-10-30T16:51:50.690030Z"
    }
   },
   "outputs": [],
   "source": [
    "# Based on the previous findings, define a number of bigrams and trigrams to be created and added to the vocabulary:\n",
    "bigrams = ['voulez-vous', 'mamma-mia', 'merry-go', 'self-confidence', 'santa-rosa', 'suzy-hang',\n",
    "           'hasta-ma√±ana', 'rock-n', 'king-kong', 'bang-boome', 'dancing-queen', 'ding-dong', 'double-cross',\n",
    "           'sup-p', 'troup-p', 'ding-dong', 'dog-gone', 'absent-minded', 'play-grind', 'ring-ring']\n",
    "trigrams = ['rock-n-roll', 'merry-go-round', 'bang-boome-boomerang', 'sup-p-per', 'troup-p-per', 'suzy-hang-around']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:51:51.582218Z",
     "start_time": "2021-10-30T16:51:51.559048Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to join adjacent words with a given sep\n",
    "def join_from_onegrams(onegram_text, true_bigr_set=bigrams, sep='-'):\n",
    "    res = []\n",
    "    skip = False\n",
    "    for prev, curr in zip(onegram_text[:-1], onegram_text[1:]):\n",
    "        if skip:\n",
    "            skip = False\n",
    "            continue\n",
    "        if f'{prev}-{curr}' in true_bigr_set:\n",
    "            res.append(f'{prev}{sep}{curr}')\n",
    "            skip = True\n",
    "        else:\n",
    "            res.append(prev)\n",
    "    if onegram_text[1:] and not skip:\n",
    "        res.append(onegram_text[-1])\n",
    "    if onegram_text and not onegram_text[1:]:\n",
    "        res = onegram_text\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:51:52.667287Z",
     "start_time": "2021-10-30T16:51:52.578713Z"
    }
   },
   "outputs": [],
   "source": [
    "df['cleaned_bigrams'] = (\n",
    "    df.cleaned_lyrics.apply(lambda x: join_from_onegrams(join_from_onegrams(x), true_bigr_set=trigrams))\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:51:53.266293Z",
     "start_time": "2021-10-30T16:51:53.239048Z"
    }
   },
   "outputs": [],
   "source": [
    "# Unique texts: get song lyrics unique words keeping the same order\n",
    "df['cleaned_lyrics_unique'] = df['cleaned_lyrics'].apply(lambda x: list(pd.unique(x)))\n",
    "df['cleaned_bigrams_unique'] = df['cleaned_bigrams'].apply(lambda x: list(pd.unique(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Compute some statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:51:53.835984Z",
     "start_time": "2021-10-30T16:51:53.826224Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vocabulary length:\n",
    "tokens = [token for token in df.cleaned_bigrams_unique for token in token]\n",
    "vocab = set(tokens)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:51:54.421981Z",
     "start_time": "2021-10-30T16:51:54.381674Z"
    }
   },
   "outputs": [],
   "source": [
    "Counter(tokens).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:51:55.043252Z",
     "start_time": "2021-10-30T16:51:55.039378Z"
    }
   },
   "outputs": [],
   "source": [
    "# LEXICAL DIVERSITY\n",
    "# Length of song lyrics in terms of unique words (bigrams included):\n",
    "df['len_bigrams_unique'] = df.cleaned_bigrams_unique.apply(lambda x: len(x))\n",
    "# proportional to:\n",
    "# df['lexical_diversity'] = df.cleaned_bigrams_unique.apply(lambda x: len(x))/len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:51:55.764626Z",
     "start_time": "2021-10-30T16:51:55.701234Z"
    }
   },
   "outputs": [],
   "source": [
    "# Top 10 songs with the highest lexical diversity \n",
    "df.iloc[df.len_bigrams_unique.sort_values(ascending=False).index].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:51:56.467777Z",
     "start_time": "2021-10-30T16:51:56.456325Z"
    }
   },
   "outputs": [],
   "source": [
    "# LEXICAL DENSITY\n",
    "# How much words are repeated in songs on average: \n",
    "def average_repetitions(text):\n",
    "    rep = Counter(text)\n",
    "    rep_values = [v for k, v in rep.items()]\n",
    "    return np.mean(rep_values)\n",
    "df['word_repetition'] = df.cleaned_bigrams.apply(lambda x: average_repetitions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:51:57.047245Z",
     "start_time": "2021-10-30T16:51:57.039785Z"
    }
   },
   "outputs": [],
   "source": [
    "df.len_bigrams_unique.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:51:57.703793Z",
     "start_time": "2021-10-30T16:51:57.697745Z"
    }
   },
   "outputs": [],
   "source": [
    "# Original lyrics length in terms of characters (will be useful in the next part):\n",
    "df['lyrics_char_len'] = df.lyrics.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T15:57:12.655357Z",
     "start_time": "2021-10-30T15:57:12.652049Z"
    }
   },
   "source": [
    "### 1.4 Additional checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:51:58.273102Z",
     "start_time": "2021-10-30T16:51:58.252399Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Explore bigrams with Phrases of Gensim\n",
    "bigram = Phrases(cleaned_text, min_count=10, threshold=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:51:58.775206Z",
     "start_time": "2021-10-30T16:51:58.761075Z"
    }
   },
   "outputs": [],
   "source": [
    "for el in bigram.vocab.items():\n",
    "    if (el[1]>10) & ('_' in el[0]):\n",
    "        print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:51:59.443780Z",
     "start_time": "2021-10-30T16:51:59.439361Z"
    }
   },
   "outputs": [],
   "source": [
    "# Word-frequency matrix\n",
    "def create_frequency_dict(text):\n",
    "    text_dict = {}\n",
    "    for token in vocab:\n",
    "        text_dict[token] = Counter(text)[token]\n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:52:02.071358Z",
     "start_time": "2021-10-30T16:52:00.166602Z"
    }
   },
   "outputs": [],
   "source": [
    "all_text_dict = []\n",
    "for text in df.cleaned_lyrics:\n",
    "    all_text_dict.append(create_frequency_dict(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:52:02.638189Z",
     "start_time": "2021-10-30T16:52:02.548909Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_matrix = pd.DataFrame(all_text_dict, index=df.track)\n",
    "tf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Emotion-based model application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:19:56.592057Z",
     "start_time": "2021-10-30T16:19:21.714383Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "classifier_bert = pipeline(\n",
    "    \"text-classification\",\n",
    "    model='bhadresh-savani/bert-base-uncased-emotion',\n",
    "    return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:34:54.236158Z",
     "start_time": "2021-10-30T16:34:54.112175Z"
    }
   },
   "outputs": [],
   "source": [
    "# test\n",
    "sample_text = \"I love using transformers. The best part is wide range of support and its easy to use\"\n",
    "sample_result = classifier_bert(sample_text)\n",
    "sample_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:34:59.032242Z",
     "start_time": "2021-10-30T16:34:59.010870Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function called in the following one\n",
    "def get_results(prediction):\n",
    "    labels_scores = [list(i.values()) for i in prediction]\n",
    "    labels = [el[0] for el in labels_scores]\n",
    "    scores = [el[1] for el in labels_scores]\n",
    "    return labels_scores, labels, scores\n",
    "\n",
    "get_results(sample_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:33:08.431336Z",
     "start_time": "2021-10-30T16:33:08.424133Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to compute the emotion scores for all songs\n",
    "def get_emotions(df, txt_col, txt_col_length, model=classifier_bert, max_length=1500):\n",
    "    emotion_labels_scores, emotion_labels, emotion_scores = [], [], []\n",
    "    for index, row in df.iterrows():\n",
    "        if isinstance(row[txt_col], str):\n",
    "            classifier_prediction = model(row[txt_col][:max_length])\n",
    "            output = get_results(classifier_prediction[0])\n",
    "            emotion_labels_scores.append(output[0])\n",
    "            emotion_labels.append(output[1])\n",
    "            emotion_scores.append(output[2])\n",
    "        else:\n",
    "            print(f'index: {index} - track name: {row.track} (no text)')\n",
    "            emotion_labels_scores.append(np.nan)\n",
    "            emotion_labels.append(np.nan)\n",
    "            emotion_scores.append([np.nan]*5)\n",
    "    return emotion_labels_scores, emotion_labels, emotion_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:47:39.876368Z",
     "start_time": "2021-10-30T16:46:11.314081Z"
    }
   },
   "outputs": [],
   "source": [
    "results_full = get_emotions(\n",
    "    df=df, \n",
    "    txt_col='lyrics', \n",
    "    txt_col_length='lyrics_char_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:52:13.979934Z",
     "start_time": "2021-10-30T16:52:13.950111Z"
    }
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results_full[2], columns=results_full[1][0], index=df.track)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:52:16.687838Z",
     "start_time": "2021-10-30T16:52:16.663550Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add the max score and emotion\n",
    "results['max_score'] = results.max(axis=1)\n",
    "results['max_emotion'] = results.idxmax(axis=1)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:52:17.852585Z",
     "start_time": "2021-10-30T16:52:17.800463Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge with the original dataset on the track id\n",
    "df = df.set_index('track')\n",
    "df.index.names = ['track_id']\n",
    "df_texts_emotions = df.merge(results, left_index=True, right_index=True)\n",
    "df_texts_emotions = df_texts_emotions.reset_index(drop=False).drop(columns='lyrics')\n",
    "df_texts_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:52:44.022816Z",
     "start_time": "2021-10-30T16:52:43.996978Z"
    }
   },
   "outputs": [],
   "source": [
    "df_texts_emotions.to_csv('../data/df_abba_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-analysis-38",
   "language": "python",
   "name": "text-analysis-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
